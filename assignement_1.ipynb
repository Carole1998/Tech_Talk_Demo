{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carole1998/Tech_Talk_Demo/blob/main/assignement_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor"
      ],
      "metadata": {
        "id": "4jQp0FV_bout"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "os6E18KuaqpX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# A 'Tensor' class in which the elements, the deltas, and a shape are stored.\n",
        "class Tensor:\n",
        "    def __init__(self, elements, deltas=None, shape=None):\n",
        "        \"\"\"\n",
        "        Initialize a Tensor with elements, deltas, and shape.\n",
        "\n",
        "        Args:\n",
        "            elements: Input data (can be single sample or batch)\n",
        "            deltas: Gradients (can be single sample or batch)\n",
        "            shape: Shape of the tensor\n",
        "        \"\"\"\n",
        "        self.elements = np.array(elements)\n",
        "        if deltas is not None:\n",
        "            self.deltas = np.array(deltas)\n",
        "        else:\n",
        "            self.deltas = None\n",
        "\n",
        "        # Handle shape\n",
        "        if shape is not None:\n",
        "            self.shape = shape\n",
        "        else:\n",
        "            if self.elements.ndim == 1:\n",
        "                self.shape = Shape(len(self.elements))\n",
        "            else:\n",
        "                self.shape = Shape(*self.elements.shape)\n",
        "\n",
        "    @property\n",
        "    def is_batch(self):\n",
        "        \"\"\"Check if tensor contains a batch of samples\"\"\"\n",
        "        return self.elements.ndim > 1\n",
        "\n",
        "    def get_batch_size(self):\n",
        "        \"\"\"Get the batch size if tensor contains a batch\"\"\"\n",
        "        return self.elements.shape[0] if self.is_batch else 1\n",
        "\n",
        "    def get_sample(self, index):\n",
        "        \"\"\"Get a single sample from the batch\"\"\"\n",
        "        if not self.is_batch:\n",
        "            return self\n",
        "        return Tensor(\n",
        "            elements=self.elements[index],\n",
        "            deltas=self.deltas[index] if self.deltas is not None else None,\n",
        "            shape=Shape(*self.elements.shape[1:])\n",
        "        )\n",
        "\n",
        "#  A shape class that specifies the dimension of the data\n",
        "class Shape:\n",
        "    def __init__(self, *args):\n",
        "        self.dimensions = args\n",
        "\n",
        "# A layer class\n",
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.layer_type = self.__class__.__name__  # Automatically get the class name as layer type\n",
        "        self.num = None  # Will be set when added to network\n",
        "\n",
        "    def forward(self, inp: Tensor) -> Tensor:\n",
        "        \"\"\"Forward pass of the layer\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, grad_out: Tensor) -> Tensor:\n",
        "        \"\"\"Backward pass of the layer\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def calc_delta_weights(self):\n",
        "        \"\"\"Calculate weight updates for the layer\"\"\"\n",
        "        pass  # Default implementation does nothing\n",
        "\n",
        "class InputLayer(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_type = \"InputLayer\"  # Explicitly set layer type\n",
        "\n",
        "    def forward(self, inp) -> Tensor:\n",
        "        \"\"\"Transform input into a Tensor\"\"\"\n",
        "        if isinstance(inp, Tensor):\n",
        "            return inp\n",
        "\n",
        "        # Convert input to Tensor if not already\n",
        "        elements = np.array(inp)\n",
        "        deltas = None\n",
        "        shape = Shape(*elements.shape) if elements.ndim > 1 else Shape(len(elements))\n",
        "        return Tensor(elements, deltas, shape)\n",
        "\n",
        "    def backward(self, grad_out: Tensor) -> Tensor:\n",
        "        \"\"\"No transformation needed in backward pass\"\"\"\n",
        "        return grad_out\n",
        "\n",
        "    def calc_delta_weights(self):\n",
        "        \"\"\"Input layer has no weights to update\"\"\"\n",
        "        pass\n",
        "\n",
        "class FullyConnectedLayer(Layer):\n",
        "    \"\"\"A fully connected layer that applies a linear transformation to the input\n",
        "    attributes:\n",
        "        input_size: the number of input neurons\n",
        "        output_size: the number of output neurons\n",
        "        weights: the weights of the layer initialized with random values in [-0.5, 0.5]\n",
        "        biases: the biases of the layer initialized with random values in [-0.5, 0.5]\n",
        "        input: the input to the layer\n",
        "        output: the output of the layer\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int, output_size: int):\n",
        "        super().__init__()\n",
        "        self.layer_type = \"FullyConnectedLayer\"  # Explicitly set layer type\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weight matrix with random values in [-0.5, 0.5]\n",
        "        self.weights = Tensor(\n",
        "            elements=np.random.uniform(-0.5, 0.5, (output_size, input_size)),\n",
        "            deltas=np.zeros((output_size, input_size)),\n",
        "            shape=Shape(output_size, input_size)\n",
        "        )\n",
        "\n",
        "        # Initialize bias vector with random values in [-0.5, 0.5]\n",
        "        self.biases = Tensor(\n",
        "            elements=np.random.uniform(-0.5, 0.5, output_size),\n",
        "            deltas=np.zeros(output_size),\n",
        "            shape=Shape(output_size)\n",
        "        )\n",
        "\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, inp: Tensor) -> Tensor:\n",
        "        \"\"\"Forward pass computes output = weights * input + biases\"\"\"\n",
        "        self.input = inp\n",
        "\n",
        "        if inp.is_batch:\n",
        "            # Batch matrix multiplication\n",
        "            output_elements = np.dot(inp.elements, self.weights.elements.T) + self.biases.elements\n",
        "        else:\n",
        "            # Single sample\n",
        "            output_elements = np.dot(self.weights.elements, inp.elements) + self.biases.elements\n",
        "\n",
        "        self.output = Tensor(\n",
        "            elements=output_elements,\n",
        "            deltas=np.zeros_like(output_elements),\n",
        "            shape=Shape(self.output_size) if not inp.is_batch else Shape(inp.get_batch_size(), self.output_size)\n",
        "        )\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, grad_out: Tensor) -> Tensor:\n",
        "        \"\"\"Backward pass computes gradients for weights, biases and input\"\"\"\n",
        "        if grad_out.is_batch:\n",
        "            # Batch processing\n",
        "            if self.input.is_batch:\n",
        "                # Accumulate weight deltas\n",
        "                self.weights.deltas += np.dot(grad_out.elements.T, self.input.elements)\n",
        "                # Accumulate bias deltas\n",
        "                self.biases.deltas += np.sum(grad_out.elements, axis=0)\n",
        "                # Compute input gradients\n",
        "                input_grads = np.dot(grad_out.elements, self.weights.elements)\n",
        "            else:\n",
        "                raise ValueError(\"Input and gradient batch sizes must match\")\n",
        "        else:\n",
        "            # Single sample\n",
        "            # Accumulate weight deltas\n",
        "            self.weights.deltas += np.outer(grad_out.elements, self.input.elements)\n",
        "            # Accumulate bias deltas\n",
        "            self.biases.deltas += grad_out.elements\n",
        "            # Compute input gradients\n",
        "            input_grads = np.dot(grad_out.elements, self.weights.elements)\n",
        "\n",
        "        return Tensor(\n",
        "            elements=input_grads,\n",
        "            deltas=None,\n",
        "            shape=self.input.shape\n",
        "        )\n",
        "\n",
        "    def calc_delta_weights(self):\n",
        "        \"\"\"Reset deltas after applying updates\"\"\"\n",
        "        self.weights.deltas = np.zeros_like(self.weights.elements)\n",
        "        self.biases.deltas = np.zeros_like(self.biases.elements)\n",
        "\n",
        "\n",
        "class ActivationLayer(Layer):\n",
        "    def __init__(self, activation_fn):\n",
        "        super().__init__()\n",
        "        self.layer_type = \"ActivationLayer\"  # Explicitly set layer type\n",
        "        self.activation_fn = activation_fn\n",
        "\n",
        "    def forward(self, inp: Tensor) -> Tensor:\n",
        "        \"\"\"Forward pass applies activation function\"\"\"\n",
        "        self.input = inp\n",
        "        output_elements = self.activation_fn.forward(inp.elements)\n",
        "        self.output = Tensor(\n",
        "            elements=output_elements,\n",
        "            deltas=np.zeros_like(output_elements),\n",
        "            shape=inp.shape\n",
        "        )\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, grad_out: Tensor) -> Tensor:\n",
        "        \"\"\"Backward pass computes gradients for activation function\"\"\"\n",
        "        input_grads = grad_out.elements * self.activation_fn.backward(self.input.elements)\n",
        "        return Tensor(elements=input_grads, deltas=None, shape=self.input.shape)\n",
        "\n",
        "    def calc_delta_weights(self):\n",
        "        \"\"\"Activation layer has no weights to update\"\"\"\n",
        "        pass\n",
        "\n",
        "class SigmoidActivation:\n",
        "    @staticmethod\n",
        "    def forward(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(x):\n",
        "        s = 1 / (1 + np.exp(-x))\n",
        "        return s * (1 - s)\n",
        "\n",
        "class SigmoidLayer(ActivationLayer):\n",
        "    def __init__(self):\n",
        "        super().__init__(SigmoidActivation())\n",
        "        self.layer_type = \"SigmoidLayer\"  # Override parent's layer type\n",
        "\n",
        "class SoftmaxLayer(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_type = \"SoftmaxLayer\"  # Explicitly set layer type\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, inp: Tensor) -> Tensor:\n",
        "        \"\"\"Forward pass applies softmax function\"\"\"\n",
        "        self.input = inp\n",
        "\n",
        "        # Subtract max for numerical stability\n",
        "        if inp.is_batch:\n",
        "            exp_elements = np.exp(inp.elements - np.max(inp.elements, axis=1, keepdims=True))\n",
        "            sum_exp = np.sum(exp_elements, axis=1, keepdims=True)\n",
        "            output_elements = exp_elements / sum_exp\n",
        "        else:\n",
        "            exp_elements = np.exp(inp.elements - np.max(inp.elements))\n",
        "            sum_exp = np.sum(exp_elements)\n",
        "            output_elements = exp_elements / sum_exp\n",
        "\n",
        "        self.output = Tensor(\n",
        "            elements=output_elements,\n",
        "            deltas=np.zeros_like(output_elements),\n",
        "            shape=inp.shape\n",
        "        )\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, grad_out) -> Tensor:\n",
        "        \"\"\"Backward pass computes gradients for softmax\"\"\"\n",
        "        if isinstance(grad_out, np.ndarray):\n",
        "            grad_out = Tensor(elements=grad_out, deltas=None, shape=Shape(len(grad_out)))\n",
        "\n",
        "        if grad_out.is_batch:\n",
        "            # Vectorized\n",
        "          input_grads = self.output.elements * (grad_out.elements - (grad_out.elements * self.output.elements).sum(axis=1, keepdims=True))\n",
        "\n",
        "        else:\n",
        "          input_grads = self.output.elements * (grad_out.elements - np.dot(grad_out.elements, self.output.elements))\n",
        "\n",
        "        return Tensor(\n",
        "            elements=input_grads,\n",
        "            deltas=None,\n",
        "            shape=self.input.shape\n",
        "        )\n",
        "\n",
        "    def calc_delta_weights(self):\n",
        "        \"\"\"Softmax layer has no weights to update\"\"\"\n",
        "        pass\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network\n",
        "\n"
      ],
      "metadata": {
        "id": "lLJVxvO0bwHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  from typing import List\n",
        "  import numpy as np\n",
        "  import os\n",
        "  import pickle\n",
        "\n",
        "  class Network:\n",
        "      def __init__(self, layers: List[Layer]=None, loss_fn=None, activation_fn=None, learning_rate=0.01):\n",
        "          \"\"\"Initialize network with layers, loss function, activation and learning rate\"\"\"\n",
        "          self.layers = layers if layers is not None else []\n",
        "          self.loss_fn = loss_fn\n",
        "          self.activation_fn = activation_fn\n",
        "          self.learning_rate = learning_rate\n",
        "\n",
        "          # Add unique identifiers to each layer for saving/loading\n",
        "          for i, layer in enumerate(self.layers):\n",
        "              layer.layer_type = layer.__class__.__name__\n",
        "              layer.num = i\n",
        "\n",
        "      def add_layer(self, layer):\n",
        "          \"\"\"Add a layer to the network and assign it a unique number\"\"\"\n",
        "          layer.num = len(self.layers)  # Assign the next available number\n",
        "          self.layers.append(layer)\n",
        "\n",
        "      def forward(self, x):\n",
        "          \"\"\"\n",
        "          Perform forward pass through all layers.\n",
        "\n",
        "          Args:\n",
        "              x: Input data\n",
        "\n",
        "          Returns:\n",
        "              Tensor: Network output\n",
        "          \"\"\"\n",
        "          current = x\n",
        "          for layer in self.layers:\n",
        "              current = layer.forward(current)\n",
        "          return current\n",
        "\n",
        "      def backward(self, grad):\n",
        "          \"\"\"\n",
        "          Perform backward pass (backpropagation) through all layers.\n",
        "\n",
        "          Args:\n",
        "              grad: Gradient from the loss function\n",
        "          \"\"\"\n",
        "          current_grad = grad\n",
        "          for layer in reversed(self.layers):\n",
        "              current_grad = layer.backward(current_grad)\n",
        "\n",
        "      def update_weights(self):\n",
        "          \"\"\"\n",
        "          Update network weights using Stochastic Gradient Descent (SGD).\n",
        "          Updates both weights and biases for all layers that have them.\n",
        "          \"\"\"\n",
        "          for layer in self.layers:\n",
        "              if hasattr(layer, 'weights'):\n",
        "                  layer.weights.elements -= self.learning_rate * layer.weights.deltas\n",
        "\n",
        "                  # Update biases using SGD\n",
        "                  layer.biases.elements -= self.learning_rate * layer.biases.deltas\n",
        "\n",
        "              layer.calc_delta_weights()\n",
        "\n",
        "      def compute_loss(self, output, target):\n",
        "          \"\"\"\n",
        "          Compute the loss between network output and target.\n",
        "          Supports both cross-entropy (for classification) and MSE (for regression).\n",
        "\n",
        "          Args:\n",
        "              output: Network output tensor\n",
        "              target: Target values tensor\n",
        "\n",
        "          Returns:\n",
        "              float: Computed loss value\n",
        "          \"\"\"\n",
        "          if isinstance(target.elements, np.ndarray) and target.elements.ndim > 1:\n",
        "              # Cross entropy for classification tasks\n",
        "              epsilon = 1e-15  # Small value to avoid log(0)\n",
        "              output_clipped = np.clip(output.elements, epsilon, 1 - epsilon)\n",
        "              return -np.sum(target.elements * np.log(output_clipped))\n",
        "          else:\n",
        "              # Mean Squared Error for regression tasks\n",
        "              return np.mean((np.array(output.elements) - np.array(target.elements)) ** 2)\n",
        "\n",
        "      def compute_loss_gradient(self, output, target):\n",
        "          \"\"\"\n",
        "          Compute the gradient of the loss function.\n",
        "\n",
        "          Args:\n",
        "              output: Network output tensor\n",
        "              target: Target values tensor\n",
        "\n",
        "          Returns:\n",
        "              numpy.ndarray: Gradient of the loss\n",
        "          \"\"\"\n",
        "          if isinstance(target.elements, np.ndarray) and target.elements.ndim > 1:\n",
        "              # Cross entropy gradient\n",
        "              return -target.elements/output.elements\n",
        "          else:\n",
        "              # MSE gradient\n",
        "              return 2 * (np.array(output.elements) - np.array(target.elements)) / len(output.elements)\n",
        "\n",
        "      def save_params(self, folder_path):\n",
        "          \"\"\"Save network parameters to files\"\"\"\n",
        "          os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "          for layer in self.layers:\n",
        "              if hasattr(layer, 'weights'):\n",
        "                  # Create unique identifier for the layer\n",
        "                  layer_id = f\"{layer.layer_type}_{layer.num}\"\n",
        "\n",
        "                  # Save weights\n",
        "                  weights_file = os.path.join(folder_path, f\"{layer_id}_weights.pkl\")\n",
        "                  with open(weights_file, 'wb') as f:\n",
        "                      pickle.dump(layer.weights.elements, f)\n",
        "\n",
        "                  # Save biases\n",
        "                  biases_file = os.path.join(folder_path, f\"{layer_id}_biases.pkl\")\n",
        "                  with open(biases_file, 'wb') as f:\n",
        "                      pickle.dump(layer.biases.elements, f)\n",
        "\n",
        "      def load_params(self, folder_path):\n",
        "          \"\"\"Load network parameters from files\"\"\"\n",
        "          for layer in self.layers:\n",
        "              if hasattr(layer, 'weights'):\n",
        "                  # Create unique identifier for the layer\n",
        "                  layer_id = f\"{layer.layer_type}_{layer.num}\"\n",
        "\n",
        "                  # Load weights\n",
        "                  weights_file = os.path.join(folder_path, f\"{layer_id}_weights.pkl\")\n",
        "                  with open(weights_file, 'rb') as f:\n",
        "                      layer.weights.elements = pickle.load(f)\n",
        "\n",
        "                  # Load biases\n",
        "                  biases_file = os.path.join(folder_path, f\"{layer_id}_biases.pkl\")\n",
        "                  with open(biases_file, 'rb') as f:\n",
        "                      layer.biases.elements = pickle.load(f)\n",
        "\n",
        "      def train_step(self, x, y, load_existing=False, save_path=None):\n",
        "          \"\"\"\n",
        "          Perform a single training step.\n",
        "\n",
        "          Args:\n",
        "              x: Input data\n",
        "              y: Target values\n",
        "              load_existing: Whether to load existing weights\n",
        "              save_path: Path to save/load weights\n",
        "\n",
        "          Returns:\n",
        "              float: Loss value for this step\n",
        "          \"\"\"\n",
        "          # Load existing weights if requested\n",
        "          if load_existing and save_path:\n",
        "              self.load_params(save_path)\n",
        "              return None\n",
        "\n",
        "          # Forward pass\n",
        "          output = self.forward(x)\n",
        "\n",
        "          # Compute loss and gradient\n",
        "          loss = self.compute_loss(output, y)\n",
        "          grad = self.compute_loss_gradient(output, y)\n",
        "\n",
        "          # Backward pass\n",
        "          self.backward(grad)\n",
        "\n",
        "          # Update weights\n",
        "          self.update_weights()\n",
        "\n",
        "          # Save if path provided\n",
        "          if save_path:\n",
        "              self.save_params(save_path)\n",
        "\n",
        "          return loss\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WxPgmimFb2QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training"
      ],
      "metadata": {
        "id": "Kzjj4ab2b-fC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import time\n",
        "import os\n",
        "\n",
        "\n",
        "def load_mnist():\n",
        "    # Load MNIST dataset\n",
        "    print(\"Loading MNIST dataset...\")\n",
        "    X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
        "\n",
        "    # Normalize pixel values to [0, 1]\n",
        "    X = X / 255.0\n",
        "\n",
        "    # Convert labels to one-hot encoding\n",
        "    encoder = OneHotEncoder(sparse_output=False)\n",
        "    y = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "    # Split into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def create_network():\n",
        "    # Create network architecture\n",
        "    network = Network(learning_rate=0.01)  # Reduced learning rate\n",
        "\n",
        "    # Input layer (784 neurons for MNIST images)\n",
        "    network.add_layer(InputLayer())\n",
        "\n",
        "    # Hidden layer 1 (256 neurons)\n",
        "    network.add_layer(FullyConnectedLayer(784, 256))\n",
        "    network.add_layer(SigmoidLayer())\n",
        "\n",
        "    # Hidden layer 2 (128 neurons)\n",
        "    network.add_layer(FullyConnectedLayer(256, 128))\n",
        "    network.add_layer(SigmoidLayer())\n",
        "\n",
        "    # Output layer (10 neurons for digits 0-9)\n",
        "    network.add_layer(FullyConnectedLayer(128, 10))\n",
        "    network.add_layer(SoftmaxLayer())\n",
        "\n",
        "    return network\n",
        "\n",
        "def evaluate_accuracy(network, X, y, batch_size=1000):\n",
        "    correct = 0\n",
        "    total = len(X)\n",
        "\n",
        "    # Process in batches for faster evaluation\n",
        "    for i in range(0, total, batch_size):\n",
        "        batch_X = X[i:i+batch_size]\n",
        "        batch_y = y[i:i+batch_size]\n",
        "\n",
        "        # Process each sample in the batch\n",
        "        x_tensor = Tensor(elements=batch_X, shape=Shape(batch_size, 784))\n",
        "        output = network.forward(x_tensor)\n",
        "        predictions = np.argmax(output.elements, axis=1)\n",
        "        actuals = np.argmax(batch_y, axis=1)\n",
        "        correct += np.sum(predictions == actuals)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "def train_mnist(epochs=10, batch_size=64):  # Reduced batch size\n",
        "    # Load data\n",
        "    X_train, X_test, y_train, y_test = load_mnist()\n",
        "\n",
        "    # Create network\n",
        "    network = create_network()\n",
        "\n",
        "    # Create results directory\n",
        "    results_dir = \"mnist_results\"\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\nStarting training...\")\n",
        "    results = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        epoch_loss = 0\n",
        "        num_batches = len(X_train) // batch_size\n",
        "\n",
        "        # Shuffle training data\n",
        "        indices = np.random.permutation(len(X_train))\n",
        "        X_train_shuffled = X_train[indices]\n",
        "        y_train_shuffled = y_train[indices]\n",
        "\n",
        "        # Mini-batch training\n",
        "        for i in range(0, len(X_train), batch_size):\n",
        "            batch_X = X_train_shuffled[i:i+batch_size]\n",
        "            batch_y = y_train_shuffled[i:i+batch_size]\n",
        "\n",
        "            # Convert batch to tensors\n",
        "            x_tensor = Tensor(elements=batch_X, deltas=None, shape=Shape(batch_size, 784))\n",
        "            y_tensor = Tensor(elements=batch_y, deltas=None, shape=Shape(batch_size, 10))\n",
        "\n",
        "            loss = network.train_step(x_tensor, y_tensor)\n",
        "            epoch_loss += loss\n",
        "\n",
        "        # Calculate average loss for epoch\n",
        "        avg_loss = epoch_loss / num_batches\n",
        "\n",
        "        # Calculate training and test accuracy (on smaller subset for speed)\n",
        "        eval_size = min(1000, len(X_train))\n",
        "        train_accuracy = evaluate_accuracy(network, X_train[:eval_size], y_train[:eval_size])\n",
        "        test_accuracy = evaluate_accuracy(network, X_test[:eval_size], y_test[:eval_size])\n",
        "\n",
        "        # Calculate epoch runtime\n",
        "        epoch_time = time.time() - epoch_start\n",
        "\n",
        "        # Store results\n",
        "        results.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'runtime': epoch_time,\n",
        "            'loss': avg_loss,\n",
        "            'train_accuracy': train_accuracy,\n",
        "            'test_accuracy': test_accuracy\n",
        "        })\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        print(f\"Runtime: {epoch_time:.2f}s\")\n",
        "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
        "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "        print(f\"Test Accuracy: {test_accuracy:.4f}\\n\")\n",
        "\n",
        "    # Save network weights\n",
        "    network.save_params(os.path.join(results_dir, \"weights\"))\n",
        "\n",
        "    # Save results to file\n",
        "    with open(os.path.join(results_dir, \"training_results.txt\"), \"w\") as f:\n",
        "        f.write(\"Epoch\\tRuntime(s)\\tLoss\\tTrain Accuracy\\tTest Accuracy\\n\")\n",
        "        for result in results:\n",
        "            f.write(f\"{result['epoch']}\\t{result['runtime']:.2f}\\t{result['loss']:.4f}\\t{result['train_accuracy']:.4f}\\t{result['test_accuracy']:.4f}\\n\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "6govXX8ob6Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mnist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzCrIQ42WOn3",
        "outputId": "3a1fc2a9-66c1-421b-cfda-4da932444a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MNIST dataset...\n",
            "\n",
            "Starting training...\n",
            "Epoch 1/10\n",
            "Runtime: 9.56s\n",
            "Average Loss: 27.6570\n",
            "Training Accuracy: 0.9520\n",
            "Test Accuracy: 0.9400\n",
            "\n",
            "Epoch 2/10\n",
            "Runtime: 9.62s\n",
            "Average Loss: 11.5308\n",
            "Training Accuracy: 0.9770\n",
            "Test Accuracy: 0.9580\n",
            "\n",
            "Epoch 3/10\n",
            "Runtime: 7.07s\n",
            "Average Loss: 8.3621\n",
            "Training Accuracy: 0.9840\n",
            "Test Accuracy: 0.9650\n",
            "\n",
            "Epoch 4/10\n",
            "Runtime: 9.56s\n",
            "Average Loss: 6.5684\n",
            "Training Accuracy: 0.9850\n",
            "Test Accuracy: 0.9710\n",
            "\n",
            "Epoch 5/10\n",
            "Runtime: 7.11s\n",
            "Average Loss: 5.3839\n",
            "Training Accuracy: 0.9880\n",
            "Test Accuracy: 0.9690\n",
            "\n",
            "Epoch 6/10\n",
            "Runtime: 9.29s\n",
            "Average Loss: 4.5206\n",
            "Training Accuracy: 0.9930\n",
            "Test Accuracy: 0.9640\n",
            "\n",
            "Epoch 7/10\n",
            "Runtime: 9.62s\n",
            "Average Loss: 3.8445\n",
            "Training Accuracy: 0.9930\n",
            "Test Accuracy: 0.9670\n",
            "\n",
            "Epoch 8/10\n",
            "Runtime: 6.99s\n",
            "Average Loss: 3.2080\n",
            "Training Accuracy: 0.9900\n",
            "Test Accuracy: 0.9770\n",
            "\n",
            "Epoch 9/10\n",
            "Runtime: 9.56s\n",
            "Average Loss: 2.7062\n",
            "Training Accuracy: 0.9950\n",
            "Test Accuracy: 0.9750\n",
            "\n",
            "Epoch 10/10\n",
            "Runtime: 9.61s\n",
            "Average Loss: 2.2863\n",
            "Training Accuracy: 0.9940\n",
            "Test Accuracy: 0.9800\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'epoch': 1,\n",
              "  'runtime': 9.56056547164917,\n",
              "  'loss': np.float64(27.657029216683352),\n",
              "  'train_accuracy': np.float64(0.952),\n",
              "  'test_accuracy': np.float64(0.94)},\n",
              " {'epoch': 2,\n",
              "  'runtime': 9.620229005813599,\n",
              "  'loss': np.float64(11.530846238585422),\n",
              "  'train_accuracy': np.float64(0.977),\n",
              "  'test_accuracy': np.float64(0.958)},\n",
              " {'epoch': 3,\n",
              "  'runtime': 7.07407283782959,\n",
              "  'loss': np.float64(8.362061521596807),\n",
              "  'train_accuracy': np.float64(0.984),\n",
              "  'test_accuracy': np.float64(0.965)},\n",
              " {'epoch': 4,\n",
              "  'runtime': 9.56212329864502,\n",
              "  'loss': np.float64(6.568394656364397),\n",
              "  'train_accuracy': np.float64(0.985),\n",
              "  'test_accuracy': np.float64(0.971)},\n",
              " {'epoch': 5,\n",
              "  'runtime': 7.1144537925720215,\n",
              "  'loss': np.float64(5.383941144777574),\n",
              "  'train_accuracy': np.float64(0.988),\n",
              "  'test_accuracy': np.float64(0.969)},\n",
              " {'epoch': 6,\n",
              "  'runtime': 9.285333633422852,\n",
              "  'loss': np.float64(4.520633843740853),\n",
              "  'train_accuracy': np.float64(0.993),\n",
              "  'test_accuracy': np.float64(0.964)},\n",
              " {'epoch': 7,\n",
              "  'runtime': 9.618232250213623,\n",
              "  'loss': np.float64(3.844451873030888),\n",
              "  'train_accuracy': np.float64(0.993),\n",
              "  'test_accuracy': np.float64(0.967)},\n",
              " {'epoch': 8,\n",
              "  'runtime': 6.988471031188965,\n",
              "  'loss': np.float64(3.207990313232793),\n",
              "  'train_accuracy': np.float64(0.99),\n",
              "  'test_accuracy': np.float64(0.977)},\n",
              " {'epoch': 9,\n",
              "  'runtime': 9.560062885284424,\n",
              "  'loss': np.float64(2.7062272117194195),\n",
              "  'train_accuracy': np.float64(0.995),\n",
              "  'test_accuracy': np.float64(0.975)},\n",
              " {'epoch': 10,\n",
              "  'runtime': 9.606355428695679,\n",
              "  'loss': np.float64(2.2862687776562685),\n",
              "  'train_accuracy': np.float64(0.994),\n",
              "  'test_accuracy': np.float64(0.98)}]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "KUsdtVKSfwp_"
      }
    }
  ]
}